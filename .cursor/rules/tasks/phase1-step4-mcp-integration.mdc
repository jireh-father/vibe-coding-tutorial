---
description:
globs:
alwaysApply: false
---
# 4단계: MCP 연동

## 🎯 목표
도구 통합 시스템 구축 및 langchain-mcp-adapters를 통한 LangGraph Agent와 MCP 도구 연동

## 📋 상세 태스크

### 4.1 MCP 환경 설정

#### requirements.txt 업데이트
```txt
# 기존 의존성...

# MCP Integration
langchain-mcp-adapters==0.1.0
mcp-client==0.1.0

# Web Scraping & Browser
playwright==1.40.0
beautifulsoup4==4.12.2
requests==2.31.0
selenium==4.15.0

# Additional utilities
aiohttp==3.9.1
```

#### 환경 변수 업데이트 (`.env.example`)
```env
# 기존 환경변수...

# MCP Configuration
MCP_SERVER_URL=localhost:3000
MCP_TIMEOUT=30

# Browser Configuration
BROWSER_HEADLESS=true
BROWSER_TIMEOUT=10

# Search Configuration
MAX_SEARCH_RESULTS=10
SEARCH_TIMEOUT=30
```

### 4.2 MCP 도구 설정

#### MCP 어댑터 설정 (`src/agent/mcp/adapter.py`)
```python
import asyncio
import os
from typing import List, Dict, Any, Optional
from langchain_mcp_adapters import MCPAdapter
from langchain.tools import BaseTool

class MCPToolAdapter:
    """MCP 도구 어댑터 클래스"""
    
    def __init__(self):
        self.mcp_adapter = None
        self.tools = []
        self._initialize_adapter()
    
    def _initialize_adapter(self):
        """MCP 어댑터 초기화"""
        try:
            server_url = os.getenv("MCP_SERVER_URL", "localhost:3000")
            timeout = int(os.getenv("MCP_TIMEOUT", "30"))
            
            self.mcp_adapter = MCPAdapter(
                server_url=server_url,
                timeout=timeout
            )
            
            # 사용 가능한 도구 로드
            self._load_tools()
            
        except Exception as e:
            print(f"MCP 어댑터 초기화 실패: {e}")
            # 폴백: 기본 도구 사용
            self._load_fallback_tools()
    
    def _load_tools(self):
        """MCP 서버에서 도구 로드"""
        try:
            # MCP 서버에서 사용 가능한 도구 목록 가져오기
            available_tools = self.mcp_adapter.list_tools()
            
            # 필요한 도구들만 필터링
            required_tools = [
                "web_search",
                "browser_navigate", 
                "extract_content",
                "price_scraper"
            ]
            
            for tool_name in required_tools:
                if tool_name in available_tools:
                    tool = self.mcp_adapter.get_tool(tool_name)
                    self.tools.append(tool)
                    
        except Exception as e:
            print(f"MCP 도구 로드 실패: {e}")
            self._load_fallback_tools()
    
    def _load_fallback_tools(self):
        """폴백 도구 로드"""
        from ..tools.search_tools import get_basic_tools
        self.tools = get_basic_tools()
    
    def get_tools(self) -> List[BaseTool]:
        """사용 가능한 도구 리스트 반환"""
        return self.tools
    
    async def close(self):
        """MCP 연결 종료"""
        if self.mcp_adapter:
            await self.mcp_adapter.close()
```

#### 웹 검색 도구 (`src/agent/tools/web_search_tools.py`)
```python
from langchain.tools import BaseTool
from typing import List, Dict, Any, Optional
import asyncio
import aiohttp
from bs4 import BeautifulSoup
from pydantic import BaseModel, Field
import re

class WebSearchInput(BaseModel):
    """웹 검색 입력 모델"""
    query: str = Field(description="검색할 키워드")
    site: Optional[str] = Field(default=None, description="특정 사이트 검색 (예: coupang.com)")
    max_results: int = Field(default=10, description="최대 검색 결과 수")

class WebSearchTool(BaseTool):
    """웹 검색 도구"""
    name = "web_search"
    description = "인터넷에서 상품 정보를 검색합니다."
    args_schema = WebSearchInput
    
    def _run(self, query: str, site: Optional[str] = None, max_results: int = 10) -> Dict[str, Any]:
        """동기 실행"""
        return asyncio.run(self._arun(query, site, max_results))
    
    async def _arun(self, query: str, site: Optional[str] = None, max_results: int = 10) -> Dict[str, Any]:
        """비동기 웹 검색 실행"""
        try:
            # 검색 쿼리 구성
            search_query = query
            if site:
                search_query = f"site:{site} {query}"
            
            # 여러 쇼핑몰에서 검색
            shopping_sites = [
                "coupang.com",
                "11st.co.kr", 
                "gmarket.co.kr",
                "auction.co.kr",
                "shopping.naver.com"
            ]
            
            all_results = []
            
            for site in shopping_sites:
                site_query = f"site:{site} {query}"
                results = await self._search_site(site_query, max_results // len(shopping_sites))
                all_results.extend(results)
            
            return {
                "query": query,
                "total_results": len(all_results),
                "results": all_results[:max_results],
                "search_sites": shopping_sites
            }
            
        except Exception as e:
            return {
                "error": str(e),
                "query": query,
                "results": []
            }
    
    async def _search_site(self, query: str, max_results: int) -> List[Dict[str, Any]]:
        """특정 사이트에서 검색"""
        # 실제 구현에서는 각 쇼핑몰의 API 또는 크롤링 로직 사용
        # 현재는 시뮬레이션 데이터 반환
        
        await asyncio.sleep(0.5)  # 검색 시뮬레이션
        
        # 시뮬레이션 결과
        mock_results = []
        for i in range(min(max_results, 3)):
            mock_results.append({
                "title": f"{query} 검색 결과 {i+1}",
                "url": f"https://example.com/product{i+1}",
                "price": 150000 + (i * 10000),
                "shop": query.split(':')[1].split()[0] if 'site:' in query else "Unknown",
                "description": f"{query} 상품 설명 {i+1}",
                "image_url": f"https://example.com/image{i+1}.jpg"
            })
        
        return mock_results

class BrowserNavigateInput(BaseModel):
    """브라우저 네비게이션 입력 모델"""
    url: str = Field(description="방문할 URL")
    wait_for: Optional[str] = Field(default=None, description="대기할 CSS 선택자")

class BrowserNavigateTool(BaseTool):
    """브라우저 네비게이션 도구"""
    name = "browser_navigate"
    description = "웹 브라우저로 특정 페이지에 접속하여 정보를 가져옵니다."
    args_schema = BrowserNavigateInput
    
    def _run(self, url: str, wait_for: Optional[str] = None) -> Dict[str, Any]:
        """동기 실행"""
        return asyncio.run(self._arun(url, wait_for))
    
    async def _arun(self, url: str, wait_for: Optional[str] = None) -> Dict[str, Any]:
        """비동기 브라우저 네비게이션"""
        try:
            # Playwright 또는 Selenium을 사용한 브라우저 자동화
            # 현재는 시뮬레이션 구현
            
            await asyncio.sleep(1)  # 페이지 로딩 시뮬레이션
            
            # 시뮬레이션 페이지 정보
            page_info = {
                "url": url,
                "title": "상품 페이지",
                "content": "상품 상세 정보...",
                "price": 150000,
                "availability": "재고 있음",
                "rating": 4.5,
                "review_count": 1234,
                "images": ["image1.jpg", "image2.jpg"],
                "description": "상품 상세 설명..."
            }
            
            return {
                "success": True,
                "page_info": page_info,
                "url": url
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "url": url
            }

class ExtractContentInput(BaseModel):
    """콘텐츠 추출 입력 모델"""
    html: str = Field(description="HTML 콘텐츠")
    selectors: Dict[str, str] = Field(description="추출할 요소의 CSS 선택자")

class ExtractContentTool(BaseTool):
    """콘텐츠 추출 도구"""
    name = "extract_content"
    description = "HTML에서 특정 정보를 추출합니다."
    args_schema = ExtractContentInput
    
    def _run(self, html: str, selectors: Dict[str, str]) -> Dict[str, Any]:
        """동기 실행"""
        return asyncio.run(self._arun(html, selectors))
    
    async def _arun(self, html: str, selectors: Dict[str, str]) -> Dict[str, Any]:
        """비동기 콘텐츠 추출"""
        try:
            soup = BeautifulSoup(html, 'html.parser')
            extracted_data = {}
            
            for key, selector in selectors.items():
                elements = soup.select(selector)
                if elements:
                    if len(elements) == 1:
                        extracted_data[key] = elements[0].get_text(strip=True)
                    else:
                        extracted_data[key] = [elem.get_text(strip=True) for elem in elements]
                else:
                    extracted_data[key] = None
            
            return {
                "success": True,
                "extracted_data": extracted_data
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": str(e)
            }

def get_mcp_tools() -> List[BaseTool]:
    """MCP 도구 리스트 반환"""
    return [
        WebSearchTool(),
        BrowserNavigateTool(),
        ExtractContentTool()
    ]
```

### 4.3 Agent 코어 업데이트

#### Agent 코어 MCP 연동 (`src/agent/core.py` 업데이트)
```python
import os
from typing import Dict, Any, List, Optional
from langchain_google_genai import ChatGoogleGenerativeAI
from langgraph.graph import StateGraph, END
from langgraph.prebuilt import ToolExecutor
from langchain.schema import BaseMessage, HumanMessage, AIMessage
from pydantic import BaseModel, Field

from .mcp.adapter import MCPToolAdapter
from .tools.web_search_tools import get_mcp_tools

class PriceFinderAgent:
    """최저가 쇼핑 Agent 메인 클래스 (MCP 연동)"""
    
    def __init__(self):
        self.llm = self._setup_llm()
        self.mcp_adapter = MCPToolAdapter()
        self.tools = self._setup_tools()
        self.tool_executor = ToolExecutor(self.tools)
        self.workflow = self._create_workflow()
        self.app = self.workflow.compile()
    
    def _setup_llm(self) -> ChatGoogleGenerativeAI:
        """Gemini LLM 설정"""
        api_key = os.getenv("GOOGLE_API_KEY")
        if not api_key:
            raise ValueError("GOOGLE_API_KEY 환경변수가 설정되지 않았습니다.")
        
        return ChatGoogleGenerativeAI(
            model=os.getenv("GEMINI_MODEL", "gemini-2.0-flash-exp"),
            google_api_key=api_key,
            temperature=0.1,
            max_tokens=2048
        )
    
    def _setup_tools(self) -> List:
        """도구 설정 (MCP 도구 우선, 폴백으로 기본 도구)"""
        try:
            # MCP 도구 사용 시도
            mcp_tools = self.mcp_adapter.get_tools()
            if mcp_tools:
                print(f"MCP 도구 {len(mcp_tools)}개 로드됨")
                return mcp_tools
        except Exception as e:
            print(f"MCP 도구 로드 실패: {e}")
        
        # 폴백: 기본 웹 검색 도구 사용
        print("기본 웹 검색 도구 사용")
        return get_mcp_tools()
    
    def _create_workflow(self) -> StateGraph:
        """LangGraph 워크플로우 생성"""
        from .workflows.search_workflow import create_search_workflow
        return create_search_workflow(self.llm, self.tool_executor)
    
    async def process_message(self, message: str, session_id: str) -> Dict[str, Any]:
        """메시지 처리 메인 메서드"""
        from .models.agent_models import AgentState
        
        initial_state = AgentState(
            messages=[HumanMessage(content=message)],
            user_query=message,
            session_id=session_id,
            current_step="start"
        )
        
        try:
            result = await self.app.ainvoke(initial_state)
            return {
                "success": True,
                "result": result,
                "session_id": session_id,
                "tools_used": [tool.name for tool in self.tools]
            }
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "session_id": session_id
            }
    
    async def close(self):
        """리소스 정리"""
        if self.mcp_adapter:
            await self.mcp_adapter.close()
```

### 4.4 실제 쇼핑몰 크롤링 도구

#### 쇼핑몰별 크롤러 (`src/agent/tools/shopping_crawlers.py`)
```python
from langchain.tools import BaseTool
from typing import List, Dict, Any, Optional
import asyncio
import aiohttp
from bs4 import BeautifulSoup
from pydantic import BaseModel, Field
import re
from urllib.parse import urljoin, quote

class ShoppingCrawlerInput(BaseModel):
    """쇼핑몰 크롤러 입력 모델"""
    query: str = Field(description="검색할 상품명")
    max_results: int = Field(default=5, description="최대 검색 결과 수")

class CoupangCrawler(BaseTool):
    """쿠팡 크롤러"""
    name = "coupang_search"
    description = "쿠팡에서 상품을 검색합니다."
    args_schema = ShoppingCrawlerInput
    
    def _run(self, query: str, max_results: int = 5) -> Dict[str, Any]:
        return asyncio.run(self._arun(query, max_results))
    
    async def _arun(self, query: str, max_results: int = 5) -> Dict[str, Any]:
        """쿠팡 검색 실행"""
        try:
            # 쿠팡 검색 URL 구성
            encoded_query = quote(query)
            search_url = f"https://www.coupang.com/np/search?q={encoded_query}"
            
            # HTTP 요청 헤더 설정
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            }
            
            async with aiohttp.ClientSession() as session:
                async with session.get(search_url, headers=headers) as response:
                    if response.status == 200:
                        html = await response.text()
                        return self._parse_coupang_results(html, max_results)
                    else:
                        return {"error": f"HTTP {response.status}", "results": []}
                        
        except Exception as e:
            return {"error": str(e), "results": []}
    
    def _parse_coupang_results(self, html: str, max_results: int) -> Dict[str, Any]:
        """쿠팡 검색 결과 파싱"""
        soup = BeautifulSoup(html, 'html.parser')
        products = []
        
        # 쿠팡 상품 리스트 선택자 (실제 선택자로 업데이트 필요)
        product_items = soup.select('.search-product')[:max_results]
        
        for item in product_items:
            try:
                # 상품 정보 추출 (실제 선택자로 업데이트 필요)
                name = item.select_one('.name')
                price = item.select_one('.price-value')
                link = item.select_one('a')
                image = item.select_one('img')
                
                if name and price:
                    product = {
                        "name": name.get_text(strip=True),
                        "price": self._extract_price(price.get_text(strip=True)),
                        "url": urljoin("https://www.coupang.com", link.get('href')) if link else "",
                        "image_url": image.get('src') if image else "",
                        "shop": "쿠팡",
                        "shipping_fee": 0  # 쿠팡은 대부분 무료배송
                    }
                    products.append(product)
                    
            except Exception as e:
                continue
        
        return {
            "shop": "쿠팡",
            "query": html[:100] + "...",  # 디버깅용
            "total_results": len(products),
            "results": products
        }
    
    def _extract_price(self, price_text: str) -> int:
        """가격 텍스트에서 숫자 추출"""
        price_numbers = re.findall(r'[\d,]+', price_text.replace(',', ''))
        return int(price_numbers[0]) if price_numbers else 0

class EleventhStreetCrawler(BaseTool):
    """11번가 크롤러"""
    name = "11st_search"
    description = "11번가에서 상품을 검색합니다."
    args_schema = ShoppingCrawlerInput
    
    def _run(self, query: str, max_results: int = 5) -> Dict[str, Any]:
        return asyncio.run(self._arun(query, max_results))
    
    async def _arun(self, query: str, max_results: int = 5) -> Dict[str, Any]:
        """11번가 검색 실행"""
        # 11번가 크롤링 로직 구현
        # 현재는 시뮬레이션 데이터 반환
        await asyncio.sleep(1)
        
        mock_products = []
        for i in range(max_results):
            mock_products.append({
                "name": f"{query} - 11번가 상품 {i+1}",
                "price": 155000 + (i * 5000),
                "url": f"https://11st.co.kr/product{i+1}",
                "image_url": f"https://11st.co.kr/image{i+1}.jpg",
                "shop": "11번가",
                "shipping_fee": 2500
            })
        
        return {
            "shop": "11번가",
            "query": query,
            "total_results": len(mock_products),
            "results": mock_products
        }

class MultiShoppingCrawler(BaseTool):
    """다중 쇼핑몰 크롤러"""
    name = "multi_shop_search"
    description = "여러 쇼핑몰에서 동시에 상품을 검색합니다."
    args_schema = ShoppingCrawlerInput
    
    def __init__(self):
        super().__init__()
        self.crawlers = [
            CoupangCrawler(),
            EleventhStreetCrawler()
            # 추가 크롤러들...
        ]
    
    def _run(self, query: str, max_results: int = 5) -> Dict[str, Any]:
        return asyncio.run(self._arun(query, max_results))
    
    async def _arun(self, query: str, max_results: int = 5) -> Dict[str, Any]:
        """다중 쇼핑몰 동시 검색"""
        try:
            # 모든 크롤러를 동시에 실행
            tasks = []
            for crawler in self.crawlers:
                task = crawler._arun(query, max_results // len(self.crawlers))
                tasks.append(task)
            
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # 결과 통합
            all_products = []
            shop_results = {}
            
            for i, result in enumerate(results):
                if isinstance(result, dict) and "results" in result:
                    shop_name = result.get("shop", f"Shop_{i}")
                    shop_results[shop_name] = result
                    all_products.extend(result["results"])
            
            # 가격순 정렬
            all_products.sort(key=lambda x: x.get("price", float('inf')))
            
            return {
                "query": query,
                "total_shops": len(self.crawlers),
                "total_products": len(all_products),
                "products": all_products[:max_results],
                "shop_results": shop_results
            }
            
        except Exception as e:
            return {
                "error": str(e),
                "query": query,
                "products": []
            }

def get_shopping_crawlers() -> List[BaseTool]:
    """쇼핑몰 크롤러 도구 리스트 반환"""
    return [
        MultiShoppingCrawler(),
        CoupangCrawler(),
        EleventhStreetCrawler()
    ]
```

### 4.5 테스트 코드 작성

#### MCP 연동 테스트 (`tests/test_agent/test_mcp_integration.py`)
```python
import pytest
import asyncio
from src.agent.mcp.adapter import MCPToolAdapter
from src.agent.tools.web_search_tools import get_mcp_tools
from src.agent.tools.shopping_crawlers import get_shopping_crawlers

@pytest.fixture
def mcp_adapter():
    """MCP 어댑터 인스턴스 생성"""
    return MCPToolAdapter()

@pytest.mark.asyncio
async def test_mcp_adapter_initialization(mcp_adapter):
    """MCP 어댑터 초기화 테스트"""
    assert mcp_adapter is not None
    assert isinstance(mcp_adapter.tools, list)

@pytest.mark.asyncio
async def test_web_search_tool():
    """웹 검색 도구 테스트"""
    tools = get_mcp_tools()
    web_search_tool = next((tool for tool in tools if tool.name == "web_search"), None)
    
    assert web_search_tool is not None
    
    result = await web_search_tool._arun("아이폰 15", max_results=3)
    assert "results" in result
    assert isinstance(result["results"], list)

@pytest.mark.asyncio
async def test_shopping_crawlers():
    """쇼핑몰 크롤러 테스트"""
    crawlers = get_shopping_crawlers()
    multi_crawler = next((tool for tool in crawlers if tool.name == "multi_shop_search"), None)
    
    assert multi_crawler is not None
    
    result = await multi_crawler._arun("갤럭시 S24", max_results=5)
    assert "products" in result
    assert isinstance(result["products"], list)

@pytest.mark.asyncio
async def test_agent_with_mcp():
    """MCP 연동된 Agent 테스트"""
    from src.agent.core import PriceFinderAgent
    
    agent = PriceFinderAgent()
    result = await agent.process_message("아이폰 15 최저가 찾아줘", "test-session")
    
    assert "success" in result
    assert "tools_used" in result
    
    # 정리
    await agent.close()
```

## ✅ 완료 기준
- [ ] langchain-mcp-adapters 설치 및 설정
- [ ] MCP 어댑터 클래스 구현
- [ ] 웹 검색 및 브라우저 도구 구현
- [ ] 실제 쇼핑몰 크롤링 도구 구현 (쿠팡, 11번가 등)
- [ ] Agent와 MCP 도구 연동 완료
- [ ] 다중 쇼핑몰 동시 검색 기능 구현
- [ ] MCP 연동 테스트 코드 작성
- [ ] 실제 상품 검색 동작 확인
- [ ] 에러 처리 및 폴백 메커니즘 구현

## 🧪 테스트 방법

### 1. MCP 도구 단독 테스트
```bash
pytest tests/test_agent/test_mcp_integration.py -v
```

### 2. 실제 쇼핑몰 검색 테스트
```bash
# 서버 실행
python scripts/run_api.py

# 실제 상품 검색 테스트
curl -X POST http://localhost:8000/api/v1/chat \
  -H "Content-Type: application/json" \
  -d '{"message": "아이폰 15 Pro 256GB 최저가 찾아줘", "session_id": "test-session"}'
```

### 3. 다중 쇼핑몰 검색 확인
```bash
# 여러 쇼핑몰 동시 검색 테스트
curl -X POST http://localhost:8000/api/v1/chat \
  -H "Content-Type: application/json" \
  -d '{"message": "삼성 갤럭시 S24 Ultra 가격 비교해줘", "session_id": "test-session"}'
```

## 🔗 다음 단계
[Phase 2 Step 5 - Streamlit 챗봇 UI 구현](mdc:.cursor/rules/tasks/phase2-step5-streamlit-ui.mdc)

## 📚 참고 문서
- [개발 태스크 계획](mdc:.cursor/rules/development-task-plan.mdc)
- [기술 아키텍처](mdc:.cursor/rules/technical-architecture.mdc)
- [API 명세서](mdc:.cursor/rules/api-specification.mdc)

