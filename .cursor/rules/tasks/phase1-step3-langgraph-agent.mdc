---
description:
globs:
alwaysApply: false
---
# 3ë‹¨ê³„: LangGraph Agent ê¸°ë³¸ ì›Œí¬í”Œë¡œìš° êµ¬í˜„

## ğŸ¯ ëª©í‘œ
AI Agent ê¸°ë³¸ êµ¬ì¡° êµ¬ì¶• ë° React Agent íŒ¨í„´ìœ¼ë¡œ ìƒí’ˆ ê²€ìƒ‰ ì›Œí¬í”Œë¡œìš° êµ¬í˜„

## ğŸ“‹ ìƒì„¸ íƒœìŠ¤í¬

### 3.1 í”„ë¡œì íŠ¸ êµ¬ì¡° í™•ì¥

```
src/agent/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ core.py
â”œâ”€â”€ workflows/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ search_workflow.py
â”‚   â””â”€â”€ price_comparison.py
â”œâ”€â”€ prompts/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ system_prompts.py
â”‚   â””â”€â”€ tool_prompts.py
â”œâ”€â”€ tools/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base_tools.py
â”‚   â””â”€â”€ search_tools.py
â””â”€â”€ models/
    â”œâ”€â”€ __init__.py
    â””â”€â”€ agent_models.py
```

### 3.2 Gemini 2.5 Flash LLM ì—°ë™

#### í™˜ê²½ ì„¤ì • ì—…ë°ì´íŠ¸ (`.env.example`)
```env
# Google Gemini API
GOOGLE_API_KEY=your_gemini_api_key_here
GEMINI_MODEL=gemini-2.0-flash-exp

# Agent Configuration
AGENT_MAX_ITERATIONS=10
AGENT_TIMEOUT=30
```

#### LLM ì„¤ì • (`src/agent/core.py`)
```python
import os
from typing import Dict, Any, List, Optional
from langchain_google_genai import ChatGoogleGenerativeAI
from langgraph.graph import StateGraph, END
from langgraph.prebuilt import ToolExecutor
from langchain.schema import BaseMessage, HumanMessage, AIMessage
from pydantic import BaseModel, Field

class AgentState(BaseModel):
    """Agent ìƒíƒœ ëª¨ë¸"""
    messages: List[BaseMessage] = Field(default_factory=list)
    user_query: str = ""
    search_results: List[Dict[str, Any]] = Field(default_factory=list)
    current_step: str = "start"
    session_id: str = ""
    error: Optional[str] = None

class PriceFinderAgent:
    """ìµœì €ê°€ ì‡¼í•‘ Agent ë©”ì¸ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.llm = self._setup_llm()
        self.tools = self._setup_tools()
        self.tool_executor = ToolExecutor(self.tools)
        self.workflow = self._create_workflow()
        self.app = self.workflow.compile()
    
    def _setup_llm(self) -> ChatGoogleGenerativeAI:
        """Gemini LLM ì„¤ì •"""
        api_key = os.getenv("GOOGLE_API_KEY")
        if not api_key:
            raise ValueError("GOOGLE_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        return ChatGoogleGenerativeAI(
            model=os.getenv("GEMINI_MODEL", "gemini-2.0-flash-exp"),
            google_api_key=api_key,
            temperature=0.1,
            max_tokens=2048
        )
    
    def _setup_tools(self) -> List:
        """ë„êµ¬ ì„¤ì • (4ë‹¨ê³„ì—ì„œ MCP ë„êµ¬ë¡œ ëŒ€ì²´ ì˜ˆì •)"""
        from .tools.search_tools import get_basic_tools
        return get_basic_tools()
    
    def _create_workflow(self) -> StateGraph:
        """LangGraph ì›Œí¬í”Œë¡œìš° ìƒì„±"""
        from .workflows.search_workflow import create_search_workflow
        return create_search_workflow(self.llm, self.tool_executor)
    
    async def process_message(self, message: str, session_id: str) -> Dict[str, Any]:
        """ë©”ì‹œì§€ ì²˜ë¦¬ ë©”ì¸ ë©”ì„œë“œ"""
        initial_state = AgentState(
            messages=[HumanMessage(content=message)],
            user_query=message,
            session_id=session_id,
            current_step="start"
        )
        
        try:
            result = await self.app.ainvoke(initial_state)
            return {
                "success": True,
                "result": result,
                "session_id": session_id
            }
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "session_id": session_id
            }
```

### 3.3 ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì‘ì„±

#### ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (`src/agent/prompts/system_prompts.py`)
```python
SYSTEM_PROMPT = """
ë‹¹ì‹ ì€ ìµœì €ê°€ ì‡¼í•‘ì„ ë„ì™€ì£¼ëŠ” ì „ë¬¸ AI Agentì…ë‹ˆë‹¤.

## ì—­í• ê³¼ ëª©í‘œ
- ì‚¬ìš©ìê°€ ì›í•˜ëŠ” ìƒí’ˆì˜ ìµœì €ê°€ë¥¼ ì°¾ì•„ ì œê³µ
- ì—¬ëŸ¬ ì‡¼í•‘ëª°ì˜ ê°€ê²©ì„ ë¹„êµí•˜ì—¬ ìµœì ì˜ êµ¬ë§¤ ì˜µì…˜ ì œì‹œ
- í• ì¸ ì •ë³´, ë°°ì†¡ë¹„, ì¿ í° ë“±ì„ ì¢…í•©ì ìœ¼ë¡œ ê³ ë ¤í•œ ì‹¤ì œ ê²°ì œ ê¸ˆì•¡ ê³„ì‚°

## ì£¼ìš” ê¸°ëŠ¥
1. **ìƒí’ˆ ê²€ìƒ‰**: ì‚¬ìš©ì ìš”ì²­ì„ ë¶„ì„í•˜ì—¬ ì •í™•í•œ ìƒí’ˆ ì‹ë³„
2. **ê°€ê²© ë¹„êµ**: ì—¬ëŸ¬ ì‡¼í•‘ëª°ì—ì„œ ë™ì¼ ìƒí’ˆì˜ ê°€ê²© ìˆ˜ì§‘ ë° ë¹„êµ
3. **ìµœì  ì¶”ì²œ**: ê°€ê²©, ì‹ ë¢°ë„, ë°°ì†¡ ì¡°ê±´ ë“±ì„ ì¢…í•©í•˜ì—¬ ìµœì  ì˜µì…˜ ì¶”ì²œ

## ì‘ë‹µ ê°€ì´ë“œë¼ì¸
- ì¹œê·¼í•˜ê³  ë„ì›€ì´ ë˜ëŠ” í†¤ìœ¼ë¡œ ì‘ë‹µ
- ê²€ìƒ‰ ê³¼ì •ì„ ë‹¨ê³„ë³„ë¡œ ì„¤ëª…
- ê°€ê²© ì •ë³´ëŠ” ì •í™•í•˜ê³  ìµœì‹  ë°ì´í„° ì œê³µ
- êµ¬ë§¤ ê²°ì •ì— ë„ì›€ì´ ë˜ëŠ” ì¶”ê°€ ì •ë³´ ì œê³µ (ë¦¬ë·°, í‰ì , ë°°ì†¡ ì •ë³´ ë“±)

## ë„êµ¬ ì‚¬ìš©
- ìƒí’ˆ ê²€ìƒ‰ì´ í•„ìš”í•œ ê²½ìš° search_products ë„êµ¬ ì‚¬ìš©
- ê°€ê²© ë¹„êµê°€ í•„ìš”í•œ ê²½ìš° compare_prices ë„êµ¬ ì‚¬ìš©
- ì¶”ê°€ ì •ë³´ê°€ í•„ìš”í•œ ê²½ìš° get_product_details ë„êµ¬ ì‚¬ìš©

í˜„ì¬ ì‚¬ìš©ìì˜ ìš”ì²­ì„ ë¶„ì„í•˜ê³  ì ì ˆí•œ ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ìµœê³ ì˜ ì‡¼í•‘ ê²½í—˜ì„ ì œê³µí•˜ì„¸ìš”.
"""

SEARCH_PROMPT = """
ì‚¬ìš©ì ìš”ì²­: {user_query}

ìœ„ ìš”ì²­ì„ ë¶„ì„í•˜ì—¬ ë‹¤ìŒì„ ìˆ˜í–‰í•˜ì„¸ìš”:

1. ìƒí’ˆëª…, ë¸Œëœë“œ, ëª¨ë¸, ì˜µì…˜ ë“±ì„ ì •í™•íˆ íŒŒì•…
2. ê²€ìƒ‰ì— í•„ìš”í•œ í‚¤ì›Œë“œ ì¶”ì¶œ
3. ì ì ˆí•œ ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ìƒí’ˆ ê²€ìƒ‰ ì‹¤í–‰

ê²€ìƒ‰ ê³¼ì •ì„ ì‚¬ìš©ìì—ê²Œ ì¹œê·¼í•˜ê²Œ ì„¤ëª…í•˜ë©´ì„œ ì§„í–‰í•˜ì„¸ìš”.
"""

COMPARISON_PROMPT = """
ê²€ìƒ‰ëœ ìƒí’ˆë“¤: {search_results}

ìœ„ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒì„ ìˆ˜í–‰í•˜ì„¸ìš”:

1. ë™ì¼ ìƒí’ˆ ì‹ë³„ ë° ê·¸ë£¹í•‘
2. ê°€ê²© ë¹„êµ (í• ì¸ê°€, ë°°ì†¡ë¹„ í¬í•¨)
3. ìµœì €ê°€ ë° ìµœì  ì˜µì…˜ ì„ ì •
4. ì‚¬ìš©ìì—ê²Œ ëª…í™•í•˜ê³  ìœ ìš©í•œ ë¹„êµ ì •ë³´ ì œê³µ

ê°€ê²© ì™¸ì—ë„ íŒë§¤ì ì‹ ë¢°ë„, ë°°ì†¡ ì¡°ê±´, ë¦¬ë·° ë“±ì„ ì¢…í•©ì ìœ¼ë¡œ ê³ ë ¤í•˜ì—¬ ì¶”ì²œí•˜ì„¸ìš”.
"""
```

#### ë„êµ¬ ì‚¬ìš© í”„ë¡¬í”„íŠ¸ (`src/agent/prompts/tool_prompts.py`)
```python
TOOL_SELECTION_PROMPT = """
ì‚¬ìš©ì ìš”ì²­: {user_query}
í˜„ì¬ ìƒí™©: {current_step}

ë‹¤ìŒ ì¤‘ ì ì ˆí•œ ë„êµ¬ë¥¼ ì„ íƒí•˜ê³  ì‚¬ìš©í•˜ì„¸ìš”:

1. **search_products**: ìƒí’ˆ ê²€ìƒ‰ì´ í•„ìš”í•œ ê²½ìš°
   - ìƒí’ˆëª…, ë¸Œëœë“œ, ëª¨ë¸ ë“±ì´ í¬í•¨ëœ ê²€ìƒ‰ ìš”ì²­
   - ì˜ˆ: "ì•„ì´í° 15 ìµœì €ê°€", "ì‚¼ì„± ì„¸íƒê¸° ê°€ê²© ë¹„êµ"

2. **compare_prices**: íŠ¹ì • ìƒí’ˆì˜ ê°€ê²© ë¹„êµê°€ í•„ìš”í•œ ê²½ìš°
   - ì´ë¯¸ ìƒí’ˆì´ ì‹ë³„ë˜ê³  ì—¬ëŸ¬ ì‡¼í•‘ëª° ê°€ê²© ë¹„êµ í•„ìš”
   - ì˜ˆ: ê²€ìƒ‰ ê²°ê³¼ê°€ ìˆê³  ê°€ê²© ë¹„êµ ìš”ì²­

3. **get_product_details**: ìƒí’ˆ ìƒì„¸ ì •ë³´ê°€ í•„ìš”í•œ ê²½ìš°
   - ë¦¬ë·°, í‰ì , ìŠ¤í™ ë“± ì¶”ê°€ ì •ë³´ ìš”ì²­
   - ì˜ˆ: "ë¦¬ë·° ì–´ë•Œ?", "ìŠ¤í™ ì•Œë ¤ì¤˜"

ë„êµ¬ë¥¼ ì‚¬ìš©í•  ë•ŒëŠ” ëª…í™•í•œ íŒŒë¼ë¯¸í„°ë¥¼ ì œê³µí•˜ì„¸ìš”.
"""

RESPONSE_FORMAT_PROMPT = """
ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì‚¬ìš©ìì—ê²Œ ì œì‹œí•  ë•Œ ë‹¤ìŒ í˜•ì‹ì„ ë”°ë¥´ì„¸ìš”:

## ğŸ” ê²€ìƒ‰ ê²°ê³¼

**ìƒí’ˆëª…**: {product_name}
**ê²€ìƒ‰ëœ ì‡¼í•‘ëª°**: {shop_count}ê°œ

### ğŸ’° ê°€ê²© ë¹„êµ

1. **ìµœì €ê°€** - {lowest_price_shop}
   - ê°€ê²©: {lowest_price}ì›
   - í• ì¸: {discount_info}
   - ë°°ì†¡: {shipping_info}
   - [êµ¬ë§¤í•˜ëŸ¬ ê°€ê¸°]({purchase_url})

2. **ë‘ ë²ˆì§¸** - {second_shop}
   - ê°€ê²©: {second_price}ì›
   - í• ì¸: {second_discount}
   - ë°°ì†¡: {second_shipping}

### ğŸ“Š ì¶”ì²œ ì´ìœ 
- {recommendation_reason}

### ğŸ’¡ ì¶”ê°€ íŒ
- {additional_tips}

ë‹¤ë¥¸ ì˜µì…˜ì„ ë³´ì‹œê±°ë‚˜ ì¶”ê°€ ì •ë³´ê°€ í•„ìš”í•˜ì‹œë©´ ë§ì”€í•´ ì£¼ì„¸ìš”!
"""
```

### 3.4 ê¸°ë³¸ ë„êµ¬ êµ¬í˜„

#### ê¸°ë³¸ ê²€ìƒ‰ ë„êµ¬ (`src/agent/tools/search_tools.py`)
```python
from langchain.tools import BaseTool
from typing import List, Dict, Any
import asyncio
from pydantic import BaseModel, Field

class SearchProductsInput(BaseModel):
    """ìƒí’ˆ ê²€ìƒ‰ ì…ë ¥ ëª¨ë¸"""
    query: str = Field(description="ê²€ìƒ‰í•  ìƒí’ˆëª… ë˜ëŠ” í‚¤ì›Œë“œ")
    category: str = Field(default="", description="ìƒí’ˆ ì¹´í…Œê³ ë¦¬ (ì„ íƒì‚¬í•­)")

class SearchProductsTool(BaseTool):
    """ìƒí’ˆ ê²€ìƒ‰ ë„êµ¬ (ì„ì‹œ êµ¬í˜„)"""
    name = "search_products"
    description = "ìƒí’ˆì„ ê²€ìƒ‰í•˜ì—¬ ì—¬ëŸ¬ ì‡¼í•‘ëª°ì˜ ê°€ê²© ì •ë³´ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."
    args_schema = SearchProductsInput
    
    def _run(self, query: str, category: str = "") -> Dict[str, Any]:
        """ë™ê¸° ì‹¤í–‰ (ë¹„ì¶”ì²œ)"""
        return asyncio.run(self._arun(query, category))
    
    async def _arun(self, query: str, category: str = "") -> Dict[str, Any]:
        """ë¹„ë™ê¸° ì‹¤í–‰"""
        # TODO: 4ë‹¨ê³„ì—ì„œ ì‹¤ì œ MCP ë„êµ¬ë¡œ ëŒ€ì²´
        # í˜„ì¬ëŠ” ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„° ë°˜í™˜
        
        await asyncio.sleep(1)  # ê²€ìƒ‰ ì‹œë®¬ë ˆì´ì…˜
        
        # ì‹œë®¬ë ˆì´ì…˜ ê²€ìƒ‰ ê²°ê³¼
        mock_results = {
            "query": query,
            "total_results": 3,
            "products": [
                {
                    "id": "product_1",
                    "name": f"{query} - ìƒí’ˆ 1",
                    "shop": "ì¿ íŒ¡",
                    "price": 150000,
                    "original_price": 180000,
                    "discount_rate": 16.7,
                    "shipping_fee": 0,
                    "url": "https://example.com/product1",
                    "rating": 4.5,
                    "review_count": 1234
                },
                {
                    "id": "product_2", 
                    "name": f"{query} - ìƒí’ˆ 2",
                    "shop": "11ë²ˆê°€",
                    "price": 155000,
                    "original_price": 180000,
                    "discount_rate": 13.9,
                    "shipping_fee": 2500,
                    "url": "https://example.com/product2",
                    "rating": 4.3,
                    "review_count": 856
                },
                {
                    "id": "product_3",
                    "name": f"{query} - ìƒí’ˆ 3", 
                    "shop": "Gë§ˆì¼“",
                    "price": 148000,
                    "original_price": 180000,
                    "discount_rate": 17.8,
                    "shipping_fee": 3000,
                    "url": "https://example.com/product3",
                    "rating": 4.2,
                    "review_count": 567
                }
            ],
            "search_time": "2024-01-01T12:00:00Z",
            "note": "ì‹¤ì œ ê²€ìƒ‰ ê¸°ëŠ¥ì€ 4ë‹¨ê³„ MCP ì—°ë™ì—ì„œ êµ¬í˜„ë©ë‹ˆë‹¤."
        }
        
        return mock_results

class ComparePricesInput(BaseModel):
    """ê°€ê²© ë¹„êµ ì…ë ¥ ëª¨ë¸"""
    products: List[Dict[str, Any]] = Field(description="ë¹„êµí•  ìƒí’ˆ ë¦¬ìŠ¤íŠ¸")

class ComparePricesTool(BaseTool):
    """ê°€ê²© ë¹„êµ ë„êµ¬"""
    name = "compare_prices"
    description = "ìƒí’ˆë“¤ì˜ ê°€ê²©ì„ ë¹„êµí•˜ì—¬ ìµœì ì˜ êµ¬ë§¤ ì˜µì…˜ì„ ì œì‹œí•©ë‹ˆë‹¤."
    args_schema = ComparePricesInput
    
    def _run(self, products: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ë™ê¸° ì‹¤í–‰"""
        return asyncio.run(self._arun(products))
    
    async def _arun(self, products: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ë¹„ë™ê¸° ì‹¤í–‰"""
        if not products:
            return {"error": "ë¹„êµí•  ìƒí’ˆì´ ì—†ìŠµë‹ˆë‹¤."}
        
        # ì‹¤ì œ ê²°ì œ ê¸ˆì•¡ ê³„ì‚° (ê°€ê²© + ë°°ì†¡ë¹„)
        for product in products:
            product["total_price"] = product["price"] + product.get("shipping_fee", 0)
        
        # ìµœì €ê°€ ìˆœìœ¼ë¡œ ì •ë ¬
        sorted_products = sorted(products, key=lambda x: x["total_price"])
        
        comparison_result = {
            "total_products": len(products),
            "lowest_price": sorted_products[0],
            "highest_price": sorted_products[-1],
            "price_difference": sorted_products[-1]["total_price"] - sorted_products[0]["total_price"],
            "sorted_products": sorted_products,
            "recommendation": {
                "best_value": sorted_products[0],
                "reason": f"ì´ {len(products)}ê°œ ì˜µì…˜ ì¤‘ ë°°ì†¡ë¹„ í¬í•¨ ìµœì €ê°€"
            }
        }
        
        return comparison_result

def get_basic_tools() -> List[BaseTool]:
    """ê¸°ë³¸ ë„êµ¬ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜"""
    return [
        SearchProductsTool(),
        ComparePricesTool()
    ]
```

### 3.5 LangGraph ì›Œí¬í”Œë¡œìš° êµ¬í˜„

#### ê²€ìƒ‰ ì›Œí¬í”Œë¡œìš° (`src/agent/workflows/search_workflow.py`)
```python
from typing import Dict, Any
from langchain.schema import BaseMessage, HumanMessage, AIMessage, SystemMessage
from langgraph.graph import StateGraph, END
from langgraph.prebuilt import ToolExecutor
from langchain_core.language_models import BaseChatModel

from ..models.agent_models import AgentState
from ..prompts.system_prompts import SYSTEM_PROMPT, SEARCH_PROMPT

def should_continue(state: AgentState) -> str:
    """ë‹¤ìŒ ë‹¨ê³„ ê²°ì •"""
    messages = state.messages
    last_message = messages[-1]
    
    # AI ë©”ì‹œì§€ì— tool_callsê°€ ìˆìœ¼ë©´ ë„êµ¬ ì‹¤í–‰
    if hasattr(last_message, 'tool_calls') and last_message.tool_calls:
        return "call_tool"
    
    # ì—ëŸ¬ê°€ ìˆìœ¼ë©´ ì¢…ë£Œ
    if state.error:
        return END
    
    # ê²€ìƒ‰ ê²°ê³¼ê°€ ìˆìœ¼ë©´ ì¢…ë£Œ
    if state.search_results:
        return END
    
    # ê¸°ë³¸ì ìœ¼ë¡œ ì¢…ë£Œ
    return END

def call_model(state: AgentState, llm: BaseChatModel) -> Dict[str, Any]:
    """LLM í˜¸ì¶œ"""
    messages = state.messages
    
    # ì‹œìŠ¤í…œ ë©”ì‹œì§€ ì¶”ê°€
    if not messages or not isinstance(messages[0], SystemMessage):
        messages = [SystemMessage(content=SYSTEM_PROMPT)] + messages
    
    response = llm.invoke(messages)
    
    return {
        "messages": messages + [response],
        "current_step": "model_response"
    }

def call_tool(state: AgentState, tool_executor: ToolExecutor) -> Dict[str, Any]:
    """ë„êµ¬ ì‹¤í–‰"""
    messages = state.messages
    last_message = messages[-1]
    
    # ë„êµ¬ ì‹¤í–‰
    tool_calls = last_message.tool_calls
    results = []
    
    for tool_call in tool_calls:
        result = tool_executor.invoke(tool_call)
        results.append(result)
    
    # ê²€ìƒ‰ ê²°ê³¼ ì €ì¥
    search_results = []
    for result in results:
        if isinstance(result, dict) and "products" in result:
            search_results.extend(result["products"])
    
    # ë„êµ¬ ì‹¤í–‰ ê²°ê³¼ë¥¼ ë©”ì‹œì§€ë¡œ ì¶”ê°€
    tool_messages = [
        AIMessage(content=f"ë„êµ¬ ì‹¤í–‰ ê²°ê³¼: {result}")
        for result in results
    ]
    
    return {
        "messages": messages + tool_messages,
        "search_results": search_results,
        "current_step": "tool_executed"
    }

def create_search_workflow(llm: BaseChatModel, tool_executor: ToolExecutor) -> StateGraph:
    """ê²€ìƒ‰ ì›Œí¬í”Œë¡œìš° ìƒì„±"""
    
    workflow = StateGraph(AgentState)
    
    # ë…¸ë“œ ì¶”ê°€
    workflow.add_node("agent", lambda state: call_model(state, llm))
    workflow.add_node("action", lambda state: call_tool(state, tool_executor))
    
    # ì‹œì‘ì  ì„¤ì •
    workflow.set_entry_point("agent")
    
    # ì¡°ê±´ë¶€ ì—£ì§€ ì¶”ê°€
    workflow.add_conditional_edges(
        "agent",
        should_continue,
        {
            "call_tool": "action",
            END: END
        }
    )
    
    # ì•¡ì…˜ í›„ ë‹¤ì‹œ ì—ì´ì „íŠ¸ë¡œ
    workflow.add_edge("action", "agent")
    
    return workflow
```

#### Agent ëª¨ë¸ (`src/agent/models/agent_models.py`)
```python
from typing import List, Dict, Any, Optional
from pydantic import BaseModel, Field
from langchain.schema import BaseMessage

class AgentState(BaseModel):
    """Agent ìƒíƒœ ëª¨ë¸"""
    messages: List[BaseMessage] = Field(default_factory=list)
    user_query: str = ""
    search_results: List[Dict[str, Any]] = Field(default_factory=list)
    current_step: str = "start"
    session_id: str = ""
    error: Optional[str] = None
    
    class Config:
        arbitrary_types_allowed = True
```

### 3.6 API ì—°ë™ ì—…ë°ì´íŠ¸

#### ìŠ¤íŠ¸ë¦¬ë° ì„œë¹„ìŠ¤ ì—…ë°ì´íŠ¸ (`src/api/services/streaming.py`)
```python
import asyncio
import json
from typing import AsyncGenerator, Dict, Any
from datetime import datetime

from ..models.chat import StreamEvent
from ...agent.core import PriceFinderAgent

class StreamingService:
    """ìŠ¤íŠ¸ë¦¬ë° ì„œë¹„ìŠ¤ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.agent = PriceFinderAgent()
    
    @staticmethod
    async def create_sse_response(event: StreamEvent) -> str:
        """Server-Sent Events í˜•ì‹ìœ¼ë¡œ ì‘ë‹µ ìƒì„±"""
        data = {
            "type": event.type,
            "data": event.data,
            "timestamp": event.timestamp.isoformat(),
            "session_id": event.session_id
        }
        return f"data: {json.dumps(data, ensure_ascii=False)}\n\n"
    
    async def stream_chat_response(
        self, 
        message: str, 
        session_id: str
    ) -> AsyncGenerator[str, None]:
        """Agentë¥¼ ì‚¬ìš©í•œ ì±„íŒ… ì‘ë‹µ ìŠ¤íŠ¸ë¦¬ë°"""
        
        # ì‹œì‘ ì´ë²¤íŠ¸
        start_event = StreamEvent(
            type="start",
            data={"message": "AI Agentê°€ ìš”ì²­ì„ ì²˜ë¦¬í•˜ê³  ìˆìŠµë‹ˆë‹¤..."},
            session_id=session_id
        )
        yield await self.create_sse_response(start_event)
        
        try:
            # Agent ì‹¤í–‰
            result = await self.agent.process_message(message, session_id)
            
            if result["success"]:
                # ì„±ê³µ ê²°ê³¼ ìŠ¤íŠ¸ë¦¬ë°
                result_event = StreamEvent(
                    type="agent_result",
                    data=result["result"],
                    session_id=session_id
                )
                yield await self.create_sse_response(result_event)
            else:
                # ì—ëŸ¬ ê²°ê³¼ ìŠ¤íŠ¸ë¦¬ë°
                error_event = StreamEvent(
                    type="error",
                    data={"error": result["error"]},
                    session_id=session_id
                )
                yield await self.create_sse_response(error_event)
        
        except Exception as e:
            # ì˜ˆì™¸ ì²˜ë¦¬
            error_event = StreamEvent(
                type="error",
                data={"error": str(e)},
                session_id=session_id
            )
            yield await self.create_sse_response(error_event)
        
        # ì™„ë£Œ ì´ë²¤íŠ¸
        complete_event = StreamEvent(
            type="complete",
            data={"message": "ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤."},
            session_id=session_id
        )
        yield await self.create_sse_response(complete_event)
```

### 3.7 í…ŒìŠ¤íŠ¸ ì½”ë“œ ì‘ì„±

#### Agent í…ŒìŠ¤íŠ¸ (`tests/test_agent/test_core.py`)
```python
import pytest
import asyncio
from src.agent.core import PriceFinderAgent

@pytest.fixture
def agent():
    """Agent ì¸ìŠ¤í„´ìŠ¤ ìƒì„±"""
    return PriceFinderAgent()

@pytest.mark.asyncio
async def test_agent_initialization(agent):
    """Agent ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸"""
    assert agent.llm is not None
    assert agent.tools is not None
    assert agent.app is not None

@pytest.mark.asyncio
async def test_process_message(agent):
    """ë©”ì‹œì§€ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸"""
    result = await agent.process_message("ì•„ì´í° 15 ìµœì €ê°€", "test-session")
    
    assert "success" in result
    assert "session_id" in result
    assert result["session_id"] == "test-session"

@pytest.mark.asyncio
async def test_search_workflow(agent):
    """ê²€ìƒ‰ ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸"""
    result = await agent.process_message("ì‚¼ì„± ê°¤ëŸ­ì‹œ S24 ê°€ê²© ë¹„êµ", "test-session")
    
    # ì„±ê³µì ìœ¼ë¡œ ì²˜ë¦¬ë˜ì—ˆëŠ”ì§€ í™•ì¸
    assert result["success"] is True or "error" in result
```

## âœ… ì™„ë£Œ ê¸°ì¤€
- [ ] Gemini 2.5 Flash LLM ì—°ë™ ì™„ë£Œ
- [ ] LangGraph Agent ê¸°ë³¸ êµ¬ì¡° êµ¬í˜„
- [ ] React Agent íŒ¨í„´ ì›Œí¬í”Œë¡œìš° êµ¬í˜„
- [ ] ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì‘ì„± (ìƒí’ˆ ë§¤ì¹­, ê°€ê²© ì •ë³´ ì •ë¦¬, ê°€ê²© ë¹„êµ)
- [ ] ê¸°ë³¸ ë„êµ¬ êµ¬í˜„ (ì„ì‹œ ê²€ìƒ‰, ê°€ê²© ë¹„êµ)
- [ ] APIì™€ Agent ì—°ë™ ì™„ë£Œ
- [ ] ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì— Agent ê²°ê³¼ í¬í•¨
- [ ] Agent í…ŒìŠ¤íŠ¸ ì½”ë“œ ì‘ì„±
- [ ] ì „ì²´ ì›Œí¬í”Œë¡œìš° ë™ì‘ í™•ì¸

## ğŸ§ª í…ŒìŠ¤íŠ¸ ë°©ë²•

### 1. í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
```bash
export GOOGLE_API_KEY="your_gemini_api_key"
```

### 2. Agent ë‹¨ë… í…ŒìŠ¤íŠ¸
```bash
pytest tests/test_agent/ -v
```

### 3. APIë¥¼ í†µí•œ Agent í…ŒìŠ¤íŠ¸
```bash
# ì„œë²„ ì‹¤í–‰
python scripts/run_api.py

# ì±„íŒ… í…ŒìŠ¤íŠ¸
curl -X POST http://localhost:8000/api/v1/chat \
  -H "Content-Type: application/json" \
  -d '{"message": "ì•„ì´í° 15 ìµœì €ê°€ ì°¾ì•„ì¤˜", "session_id": "test-session"}'
```

## ğŸ”— ë‹¤ìŒ ë‹¨ê³„
[Phase 1 Step 4 - MCP ì—°ë™](mdc:.cursor/rules/tasks/phase1-step4-mcp-integration.mdc)

## ğŸ“š ì°¸ê³  ë¬¸ì„œ
- [ê°œë°œ íƒœìŠ¤í¬ ê³„íš](mdc:.cursor/rules/development-task-plan.mdc)
- [ê¸°ìˆ  ì•„í‚¤í…ì²˜](mdc:.cursor/rules/technical-architecture.mdc)
- [ì œí’ˆ ìš”êµ¬ì‚¬í•­](mdc:.cursor/rules/product-requirements-document.mdc)

